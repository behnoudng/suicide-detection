# Suicide Risk Detection

A hybrid ensemble model combining XGBoost and fine-tuned BERT for detecting suicidal ideation in text. Optimized for recall, because missing someone in crisis is not an acceptable error.

![Python](https://img.shields.io/badge/Python-3.10+-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.9-orange)
![XGBoost](https://img.shields.io/badge/XGBoost-3.1-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

---

## Performance

| Model        | Recall    | Precision | F1-Score |
| ------------ | --------- | --------- | -------- |
| XGBoost      | 0.894     | â€”         | â€”        |
| BERT         | 0.985     | â€”         | â€”        |
| **Ensemble** | **0.988** | 0.963     | 0.975    |
|              |           |           |          |

Final ensemble achieves **98% accuracy** with a decision threshold tuned for high recall (0.4).

---
## Why Recall?
In suicide risk detection, a false negative means a person in crisis goes undetected. A false positive means extra review. The cost asymmetry is obvious. This model is tuned accordingly.

---

## Architecture


                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Input Text    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                             â”‚
              â–¼                             â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   Preprocessing â”‚          â”‚   Raw Text      â”‚
     â”‚   + TF-IDF      â”‚          â”‚   (for BERT)    â”‚
     â”‚   + Features    â”‚          â”‚                 â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                             â”‚
              â–¼                             â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚    XGBoost      â”‚          â”‚   Fine-tuned    â”‚
     â”‚                 â”‚          â”‚   BERT          â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                             â”‚
              â”‚      P(suicide)             â”‚ P(suicide)
              â”‚         0.3                 â”‚    0.7
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Weighted Averageâ”‚
                    â”‚   Threshold=0.4 â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Prediction    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

**Ensemble weights:** 30% XGBoost, 70% BERT

---

## Features

### Text Preprocessing
- URL, email, and Reddit-specific notation removal
- Lemmatization with NLTK
- Stopword removal *with negation preservation* (`not`, `never`, `nothing`, etc. are kept. They matter here)

### Engineered Features (XGBoost)
| Feature Type | Examples |
|--------------|----------|
| Structural | text length, word count |
| Punctuation | `!`, `?`, `...` counts |
| Linguistic | first-person pronoun ratio, negation ratio |
| Domain-specific | death-related words (`die`, `kill`, `suicide`, `end`) |
| Affect | sadness indicators (`hopeless`, `empty`, `lonely`) |

### TF-IDF
- 15,000 features
- 1-3 ngrams
- Sublinear term frequency scaling

---

## Installation
```bash
bash
git clone https://github.com/behnoudng/suicide-detection.git
cd suicide-detection
pip install -r requirements.txt
```

### Requirements (key dependencies)
- Python 3.10+
- PyTorch 2.9
- Transformers 4.57
- XGBoost 3.1
- scikit-learn 1.7
- NLTK 3.9

---

## Usage
### Training Pipeline

Run in order:
```bash
# 1. Prepare and split data
python src/data_prep.py

# 2. Preprocess text
python src/text_preprocessing.py

# 3. Generate features
python src/feature_engineering.py

# 4. Train XGBoost
python src/train_xgboost.py

# 5. Fine-tune BERT (see notebook)
# I recommend using Google Colab's free GPU
jupyter notebook notebooks/finetune_bert.ipynb

# 6. Run ensemble evaluation
python src/ensemble_model.py
```
### Inference
```python
from src.ensemble_model import HybridEnsemble
from scipy.sparse import load_npz

ensemble = HybridEnsemble(
    xgboost_path='data/models/xgboost_model.pkl',
    bert_path='data/models/suicide_bert_final',
    weights=(0.3, 0.7)
)

# For single prediction, you'll need to preprocess and featurize first
# See src/feature_engineering.py for the pipeline
prediction = ensemble.predict(X_features, texts, threshold=0.4)
```


---

## Project Structure

â”œâ”€â”€ api/                    # FastAPI backend (coming soon)
â”œâ”€â”€ app/                    # Streamlit interface (coming soon)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Original dataset
â”‚   â”œâ”€â”€ processed/          # Preprocessed splits
â”‚   â””â”€â”€ models/             # Trained models & artifacts
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb
â”‚   â”œâ”€â”€ 02_modeling.ipynb
â”‚   â”œâ”€â”€ 03_evaluation.ipynb
â”‚   â””â”€â”€ finetune_bert.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_prep.py
â”‚   â”œâ”€â”€ text_preprocessing.py
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ train_xgboost.py
â”‚   â”œâ”€â”€ ensemble_model.py
â”‚   â””â”€â”€ tune_ensemble.py
â””â”€â”€ tests/

---
## API & Interface

ğŸš§ **Coming Soon**

- RESTful API with FastAPI
- Streamlit demo interface
---

## Dataset

This project uses the [Suicide and Depression Detection Dataset](https://www.kaggle.com/datasets/nikhileswarkomati/suicide-watch) licensed under CC BY-SA 4.0.

- **Source:** Reddit posts from r/SuicideWatch and r/depression (labeled "suicide") vs. other subreddits (labeled "non-suicide")
- **Size:** ~232K samples
- **Balance:** Roughly 50/50 split

---

## Limitations

- Trained on Reddit data. May not generalize to other platforms or clinical text
- English only
- Not a substitute for professional mental health assessment

---

## License
MIT

---

## Acknowledgments

- Dataset by [Nikhileswar Komati](https://www.kaggle.com/nikhileswarkomati)
- BERT base model from Hugging Face

---

